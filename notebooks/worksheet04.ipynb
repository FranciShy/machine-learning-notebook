{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 4\n",
    "## Logistic regression\n",
    "***\n",
    "In this workshop we'll be implementing L2-regularised logistic regression using `scipy` and `numpy`. \n",
    "Our key objectives are:\n",
    "\n",
    "* to become familiar with the optimisation problem that sits behind L2-regularised logistic regression;\n",
    "* to apply polynomial basis expansion and recognise when it's useful; and\n",
    "* to experiment with the effect of L2 regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary classification data\n",
    "Let's begin by generating some binary classification data.\n",
    "To make it easy for us to visualise the results, we'll stick to a two-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_RESOLUTION = 128\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "X, Y = make_circles(n_samples=500, noise=0.17, factor=0.65, random_state=90051)\n",
    "plt.plot(X[Y==0,0], X[Y==0,1], '.', label = \"$y=0$\", c='r')\n",
    "plt.plot(X[Y==1,0], X[Y==1,1], '.', label = \"$y=1$\", c='g')\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's interesting about this data? Do you think logistic regression will perform well?\n",
    "\n",
    "**Answer:** *This question is answered in section 3.*\n",
    "\n",
    "In preparation for fitting and evaluating a logistic regression model, we randomly partition the data into train/test sets. We use the `train_test_split` function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "In binary classification we receive training data $\\mathcal{D} = \\left((\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\right)$, where $\\mathbf{x}_k \\in \\mathbb{R}^N$ denotes the feature vector associated with the $k$th training point and the targets $y \\in \\{0,1\\}$. Logistic regression models the distribution of the binary target $y$ *conditional* on the feature vector $\\mathbf{x}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "y | \\mathbf{x} \\sim \\mathrm{Bernoulli}[\\sigma(\\mathbf{w}^T \\mathbf{x} + b)]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^N$ is the weight vector, $b \\in \\mathbb{R}$ is the bias term and $\\sigma(z) = 1/(1 + e^{-z})$ is the logistic function. Note here our parameter of interest $\\theta$ is the conditional probability of a particular instance belonging to class 1 given observation of the associated feature vector $\\mathbf{x}$:\n",
    "\n",
    "$$\\theta = p(y = 1 \\vert \\mathbf{x}) = \\sigma\\left(\\mathbf{w}^T \\mathbf{x} + b\\right) $$\n",
    "To simplify the notation, we'll collect the model parameters $\\mathbf{w}$ and $b$ in a single vector $\\mathbf{v} = [b, \\mathbf{w}]$. \n",
    "\n",
    "To find appropriate parameters $\\mathbf{v}$, we want to maximize the log-likelihood with respect to $\\mathbf{v}$, in lecture it was shown this is equivalent to minimization of the sum of cross-entropies over the instances ($i = 1,\\ldots,n$) in the training set\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y}) = -\\log \\prod_{i=1}^n p\\left(y_i \\vert \\mathbf{x}_i\\right) = - \\sum_{i = 1}^{n} \\left\\{ y_i \\log p(y=1 \\vert \\mathbf{x}) + (1 - y_i) \\log p (y=0 \\vert \\mathbf{x}) \\right\\}\n",
    "$$\n",
    "\n",
    "Often an L2 regularisation term of the form $\\mathcal{L}_{\\mathrm{reg}}(\\mathbf{w}) = \\frac{1}{2} \\lambda \\mathbf{w}^T \\mathbf{w}$ is added to the objective to penalize large weights (this can help prevent overfitting to idiosycrancies in the training set). Note that $\\lambda \\geq 0$ controls the strength of the regularisation term.\n",
    "\n",
    "Putting this together, our goal is to minimise the following objective function with respect to $\\mathbf{v}$:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y}) = \\mathcal{L}_\\mathrm{reg}(\\mathbf{w}) + \\mathcal{L}_{CE}(\\mathbf{v}; \\mathbf{x}, \\mathbf{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### **Exercise 1 (Discussion?):** \n",
    "The L2 regularization term  $\\mathcal{L}_{\\mathrm{reg}}(\\mathbf{w}) = \\frac{1}{2} \\lambda \\mathbf{w}^T \\mathbf{w}$ is commonly said to reduce overfitting. Give a brief justification why?\n",
    "\n",
    "#### **Exercise 2 (Discussion?):** \n",
    "Why do we only include the weights $\\mathbf{w}$ in the L2 regularization term? i.e. the bias terms are excluded from regularization.\n",
    "\n",
    "#### **Exercise 3:**\n",
    "Given we model the conditional probability of label $y=1$ to be $p(y = 1 \\vert \\mathbf{x}) = \\sigma\\left(\\mathbf{w}^T \\mathbf{x} + b\\right)$, show that prediction is based on a linear decision rule given by the sign of logarithm of the ratio of probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\log \\frac{p(y=1 \\vert \\mathbf{x})}{p(y=0 \\vert \\mathbf{x})} = \\mathbf{w}^T \\mathbf{x} + b\n",
    "\\end{equation}\n",
    "\n",
    "This is why logistic regression is referred to as a _log-linear model_. What is the decision boundary for logistic regression? \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to find a solution to this minimisation problem using the BFGS algorithm (named after the inventors Broyden, Fletcher, Goldfarb and Shanno). BFGS is a \"hill-climbing\" algorithm like gradient descent, however it additionally makes use of second-order derivative information (by approximating the Hessian). It converges in fewer iterations than gradient descent (it's convergence rate is *superlinear* whereas gradient descent is only *linear*).\n",
    "\n",
    "We'll use an implementation of BFGS provided in `scipy` called `fmin_bfgs`. The algorithm requires two functions as input: (i) a function that evaluates the objective $f(\\mathbf{v}; \\ldots)$ and (ii) a function that evalutes the gradient $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$.\n",
    "\n",
    "Let's start by writing a function to compute $f(\\mathbf{v}; \\ldots)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # this is the logistic function\n",
    "sigmoid = expit\n",
    "\n",
    "# v: parameter vector\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "def objective(v, X, Y, Lambda, L2=True):\n",
    "    #  Implement the logistic regression objective here\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    v:            Parameter vector, shape [n+1]\n",
    "                  Should contain bias term as zeroth element\n",
    "    X:            Feature matrix, shape [N, n]\n",
    "    Y:            Labels, shape [N]\n",
    "    \n",
    "    Outputs:\n",
    "    loss:         (Scalar) objective value\n",
    "    \"\"\"\n",
    "    epsilon = 1e-5  # numerical stability\n",
    "    prob_1 = ...  # fill in\n",
    "    L2_loss = ... # fill in\n",
    "    cross_entropy = ...  # fill in\n",
    "    \n",
    "    if L2 is True:\n",
    "        loss = cross_entropy + L2_loss\n",
    "    else:\n",
    "        loss = cross_entropy\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradient, we use the following result (if you're familiar with vector calculus, you may wish to derive this yourself):\n",
    "$$\n",
    "\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots) = \\left[\\frac{\\partial f(\\mathbf{w}, b;\\ldots)}{\\partial b}, \\nabla_{\\mathbf{w}} f(\\mathbf{w}, b; \\ldots) \\right] = \\left[\\sum_{i = 1}^{n} \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i, \\lambda \\mathbf{w} + \\sum_{i = 1}^{n} (\\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i)\\mathbf{x}_i\\right]\n",
    "$$\n",
    "\n",
    "The function below implements $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v: parameter vector\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "def grad_objective(v, X, Y, Lambda):\n",
    "    prob_1 = sigmoid(np.dot(X, v[1::]) + v[0])\n",
    "    grad_b = np.sum(prob_1 - Y)\n",
    "    grad_w = Lambda * v[1::] + np.dot(prob_1 - Y, X)\n",
    "    return np.insert(grad_w, 0, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll write a function to compute the solution through vanilla gradient descent. Recall that we follow the negative of the gradient in parameter space to update our parameters. Recall the basic update loop is as follows:\n",
    "\n",
    "$$ \\bf{w}_{t+1} = \\bf{w}_t - \\eta_t \\nabla_w \\mathcal{L}(\\bf{w}) $$\n",
    "\n",
    "Note that this simple procedure is the workhorse of many non-convex optimization programs, such as those used in modern neural network libraries. Contemporary libraries add more spice to the exact update, but the core idea of traversing some loss landscape in the direction of the negative gradient remains the same.\n",
    "\n",
    "Typically we halt when some stopping condition is reached, in this case we'll stop when the 2-norm of the difference between iterates drops below some threshold $\\epsilon$.\n",
    "\n",
    "$$ \\Vert \\bf{w}_{t+1} - \\bf{w}_t \\Vert^2 \\leq \\epsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gd(X, Y, Lambda, v_initial, eta=0.01, iterations=42, L2=True, EPS=1e-5):\n",
    "    \"\"\"\n",
    "    X: feature matrix\n",
    "    Y: class labels\n",
    "    Lambda: regularisation constant\n",
    "    v_initial: initial guess for parameter vector\n",
    "    eta: Learning rate\n",
    "    iterations: Number of GD iterations\n",
    "    \"\"\"\n",
    "    if L2 is False:\n",
    "        Lambda = 0.0\n",
    "    \n",
    "    v_history = [v_initial]\n",
    "    grad_history = [grad_objective(v_initial, X, Y, Lambda)]\n",
    "    v = v_initial\n",
    "\n",
    "    for itr in range(iterations):\n",
    "        \n",
    "        grad_v = ...  # fill in \n",
    "        v = ...  # fill in\n",
    "        \n",
    "        # Stopping condition\n",
    "        if np.linalg.norm(v-v_history[-1]) < EPS:\n",
    "            print('Ok, past iterates are pretty similar, I can stop now!')\n",
    "            break\n",
    "            \n",
    "        print('v:', v, ' | Objective: {:.3f}'.format(objective(v, X, Y, Lambda)), '| Gradient 2-norm: {:.3f}'.format(np.linalg.norm(grad_v)))    \n",
    "        v_history.append(v)\n",
    "        grad_history.append(grad_v)\n",
    "        \n",
    "    return v, v_history, grad_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out! We'll guess an initial parameter vector of $v_0 = (3,1,4)$.\n",
    "\n",
    "**Exercise:** Investigate how different initializations of $v_0$ and learning rates $\\eta$ change the convergence of the algorithm, by reference to the contour plot below. If you are familiar with optimization theory, you may know that convergence of GD on is only guaranteed for an appropriately decaying learning rate $\\eta_t$. You may want to decay $\\eta_t$ with time, e.g. $\\eta_t = C/(t+1), C/(t+1)^2$, etc. for some constant $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1\n",
    "v_opt, v_history, grad_history = logistic_regression_gd(X_train, Y_train, Lambda, v_initial=(3,1,4), eta=0.01)\n",
    "w = v_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the magnitude of the gradient decays as we approach the optimum. This is expected, $\\nabla f$ is the direction of the steepest rate of change, so $\\Vert \\nabla f \\Vert$ is the maximum rate of change, which should decrease as we approach the optimum of an approximately convex function. To visualize the loss landscape, we'll plot a contour plot of the loss function as well as some intermediate solutions for the weights $w_1$, $w_2$. This allows us to examine how the objective changes as a function of the weights. Note that we exclude the bias term otherwise this would not be possible to visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.025\n",
    "eta = 0.4\n",
    "w1_limit = 4\n",
    "w2_limit = 4\n",
    "w1, w2 = np.meshgrid(np.arange(w[1]-w1_limit, w[1]+w1_limit, delta), np.arange(w[2]-w2_limit, w[2]+w2_limit, delta))\n",
    "ws = np.vstack([w[0] * np.ones_like(w1.flatten()), w1.flatten(), w2.flatten()])\n",
    "obj = np.zeros(ws.shape[1])\n",
    "for i in range(ws.shape[1]):\n",
    "    obj[i] = objective(ws[:,i], X_train, Y_train, Lambda=1.0)\n",
    "    \n",
    "plt.figure()\n",
    "CS = plt.contour(w1, w2, obj.reshape(w1.shape), levels=np.exp(np.linspace(np.log(min(obj.flatten())), np.log(max(obj.flatten())), 8)))\n",
    "\n",
    "# Plot minimum returned by GD\n",
    "plt.plot(w[1], w[2], 'rx')\n",
    "norm = lambda x: x/np.sqrt(np.sum(np.square(x)))\n",
    "# Plot sequence of parameter vectors calculated via GD\n",
    "for grad_v, v in zip(grad_history[:10], v_history[:10]):\n",
    "    plt.scatter(v[1], v[2], s=20, c='b', alpha=0.25)\n",
    "    \n",
    "for i in range(len(v_history)-1):\n",
    "    plt.plot([v_history[i][1], v_history[i+1][1]], [v_history[i][2], v_history[i+1][2]], lw=0.5, ls='--')\n",
    "    \n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.title('Contour plot of objective function in weight space - GD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solving the minimization problem using BFGS\n",
    "\n",
    "Gradient descent only uses first order derivative information to update parameters (it extrapolates from the current parameter value using a linear approximation to the function). Second-order methods such as BFGS can use the second-order derivative information to increase the rate of convergence. BFGS constructs an approximation to the Hessian matrix of second derivatives at each iteration, and uses this information to guide the next update. However such methods leveraging Hessian information tend to require a high space complexity and are only used for problems with a relatively low number of parameters.\n",
    "\n",
    "Now that we've implemented functions to compute the objective and the gradient, we can plug them into `fmin_bfgs`.\n",
    "Specifically, we define a function `my_logistic_regression` which calls `fmin_bfgs` and returns the optimal weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "# v_initial: initial guess for parameter vector\n",
    "def logistic_regression_bfgs(X, Y, Lambda, v_initial, disp=True, L2=True):\n",
    "    \n",
    "    if L2 is False:\n",
    "        Lambda = 0.0\n",
    "    \n",
    "    v_history = [v_initial]\n",
    "    grad_history = [grad_objective(v_initial, X, Y, Lambda)]\n",
    "    \n",
    "    def display(v):\n",
    "        # Function for displaying progress\n",
    "        grad = grad_objective(v, X, Y, Lambda)\n",
    "        print('v:', v, ' | Objective: {:.3f}'.format(objective(v, X, Y, Lambda)), '| Gradient 2-norm: {:.3f}'.format(np.linalg.norm(grad)))\n",
    "        v_history.append(v)\n",
    "        grad_history.append(grad)\n",
    "    \n",
    "    return fmin_bfgs(f=objective, fprime=grad_objective, \n",
    "                     x0=v_initial, args=(X, Y, Lambda), disp=disp, \n",
    "                     callback=display), v_history, grad_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1\n",
    "v_initial = np.zeros(X_train.shape[1] + 1) # fill in a vector of zeros of appropriate length\n",
    "# Hint: how many parameters in our model?\n",
    "v_opt, v_history, grad_history = logistic_regression_bfgs(X_train, Y_train, Lambda, v_initial=(3,1,4))\n",
    "w = v_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot a contour plot of the loss function as well as some intermediate solutions for the weights $w_1$, $w_2$. You should observe that this exhibits much faster convergence than vanilla GD. Unfortunately, due to the added complexity of the algorithm, approximate second order methods such as BFGS are only viable for problems with a relatively small number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.025\n",
    "eta = 0.4\n",
    "w1_limit = 4\n",
    "w2_limit = 4\n",
    "w1, w2 = np.meshgrid(np.arange(w[1]-w1_limit, w[1]+w1_limit, delta), np.arange(w[2]-w2_limit, w[2]+w2_limit, delta))\n",
    "ws = np.vstack([w[0] * np.ones_like(w1.flatten()), w1.flatten(), w2.flatten()])\n",
    "obj = np.zeros(ws.shape[1])\n",
    "for i in range(ws.shape[1]):\n",
    "    obj[i] = objective(ws[:,i], X_train, Y_train, Lambda=1.0)\n",
    "    \n",
    "plt.figure()\n",
    "CS = plt.contour(w1, w2, obj.reshape(w1.shape), levels=np.exp(np.linspace(np.log(min(obj.flatten())), np.log(max(obj.flatten())), 8)))\n",
    "\n",
    "# Plot minimum returned by BFGS\n",
    "plt.plot(w[1], w[2], 'rx')\n",
    "norm = lambda x: x/np.sqrt(np.sum(np.square(x)))\n",
    "# Plot sequence of parameter vectors calculated via BFGS\n",
    "for grad_v, v in zip(grad_history[:6], v_history[:6]):\n",
    "    plt.scatter(v[1], v[2], s=20, c='b', alpha=0.25)\n",
    "#     w1_grad, w2_grad = grad_v[1], grad_v[2]\n",
    "#     w1_grad, w2_grad = norm(w1_grad), norm(w2_grad)\n",
    "    # plt.plot([v[1], v[1] - eta * w1_grad], [v[2], v[2] - eta * w2_grad])\n",
    "for i in range(len(v_history)-1):\n",
    "    plt.plot([v_history[i][1], v_history[i+1][1]], [v_history[i][2], v_history[i+1][2]], lw=0.5, ls='--')\n",
    "    \n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.title('Contour plot of objective function in weight space - BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data points and decision boundary\n",
    "def plot_results(X, Y, v, trans_func=None):\n",
    "    # Scatter plot in feature space\n",
    "    plt.plot(X[Y==0,0], X[Y==0,1], '.', label = \"$y=0$\", c='r')\n",
    "    plt.plot(X[Y==1,0], X[Y==1,1], '.', label = \"$y=1$\", c='g')\n",
    "    \n",
    "    # Compute axis limits\n",
    "    x0_lower = X[:,0].min() - 0.1\n",
    "    x0_upper = X[:,0].max() + 0.1\n",
    "    x1_lower = X[:,1].min() - 0.1\n",
    "    x1_upper = X[:,1].max() + 0.1\n",
    "    \n",
    "    # Generate grid over feature space\n",
    "    x0, x1 = np.mgrid[x0_lower:x0_upper:.01, x1_lower:x1_upper:.01]\n",
    "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    if (trans_func is not None):\n",
    "        grid = trans_func(grid) # apply transformation to features\n",
    "    arg = (np.dot(grid, v[1::]) + v[0]).reshape(x0.shape)\n",
    "    \n",
    "    # Plot decision boundary (where w^T x + b == 0)\n",
    "    plt.contour(x0, x1, arg, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
    "    plt.legend()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.show()\n",
    "\n",
    "plot_results(X, Y, v_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the solution what you expected? Is it a good fit for the data?\n",
    "\n",
    "**Answer:** *It's not a good fit because logistic regression is a linear classifier, and the data is not linearly seperable.*\n",
    "\n",
    "**Question:** What's the accuracy of this model? Fill in the code below assuming the following decision function\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\begin{cases}\n",
    "        1, \\mathrm{if} \\ p(y = 1|\\mathbf{x}) \\geq \\frac{1}{2}, \\\\\n",
    "        0, \\mathrm{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_predictions(v_opt, X_test):\n",
    "    #  Implement the logistic regression prediction rule here\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    v_opt:        Optimized weights, shape [n+1]\n",
    "                  Should contain bias term as zeroth element\n",
    "    X_test:       Testing data, shape [N_test, n]\n",
    "    \n",
    "    Outputs:\n",
    "    Y_test_pred:  Vector of predictions for given test instances\n",
    "    \"\"\"\n",
    "    w = ...\n",
    "    b = ...\n",
    "    y_prob = ...\n",
    "    Y_test_pred = ...\n",
    "    \n",
    "    return Y_test_pred\n",
    "\n",
    "Y_test_pred = get_predictions(v_opt, X_test)\n",
    "\n",
    "print('Accuracy achieved: {:.3f}'.format(accuracy_score(Y_test, Y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing nice docstrings allows your code to be self-documenting! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding polynomial features\n",
    "\n",
    "We've seen that ordinary logistic regression does poorly on this data set, because the data is not linearly separable in the $x_0,x_1$ feature space.\n",
    "\n",
    "We can get around this problem using basis expansion. In this case, we'll augment the feature space by adding polynomial features of degree 2. In other words, we replace the original feature matrix $\\mathbf{X}$ by a transformed feature matrix $\\mathbf{\\Phi}$ which contains additional columns corresponding to $x_0^2$, $x_0 x_1$ and $x_1^2$. This is done using the function `add_quadratic_features` defined below.\n",
    "\n",
    "**Note:** There's a built-in function in `sklearn` for adding polynomial features located at `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: original feature matrix\n",
    "def add_quadratic_features(X):\n",
    "    return np.column_stack([X, X[:,0]**2, X[:,0]*X[:,1], X[:,1]**2])\n",
    "\n",
    "def add_cubic_features(X):\n",
    "    u, v= X[:,0], X[:,1]\n",
    "    return np.column_stack([X, u**2, u*v, v**2, u**2*v, u*v**2, u**3, v**3])\n",
    "\n",
    "Phi_train = add_quadratic_features(X_train)\n",
    "Phi_test = add_quadratic_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the shape of the augmented feature matrix $\\Phi$, is it what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'Shape of Phi:', Phi_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our custom logistic regression function again on the augmented feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 10**(-3)\n",
    "v_initial = np.zeros(Phi_train.shape[1] + 1) # fill in a vector of zeros of appropriate length\n",
    "v_opt_phi, v_history, grad_history = logistic_regression_bfgs(Phi_train, Y_train, Lambda, v_initial)\n",
    "plot_results(X, Y, v_opt_phi, trans_func=add_quadratic_features)\n",
    "# plot_results(X, Y, v_opt_phi, trans_func=add_cubic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we should get a better result for the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "w_phi = v_opt_phi[1:]\n",
    "b_phi = v_opt_phi[0]\n",
    "logits = ...  # The argument of the sigmoid / expit operation\n",
    "Y_test_pred = ...  # How can we get the predicted class from the output logits?\n",
    "print('Accuracy achieved: {:.3f}'.format(accuracy_score(Y_test, Y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Effect of regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've fixed the regularisation constant so that $\\lambda = 1$. (Note it's possible to choose an \"optimal\" value for $\\lambda$ by applying cross-validation.)\n",
    "\n",
    "**Question:** What do you think will happen if we switch the regularisation off? Try setting $\\lambda$ to a small value (say $10^{-3}$) and check whether the accuracy of the model is affected. You may wish to scan across a range of values for $\\lambda$ and observe which value gives you the best accuracy on the test set. You may want to experiment with cubic or another monomial features.\n",
    "\n",
    "**Answer:** *Generally speaking, we risk overfitting if the regularisation constant is too small (or switched off entirely). Whether regularization helps with generalization may depend on the size of your training dataset and the features considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some insight into the optimisation problem behind logistic regression, you should feel confident in using the built-in implementation in `sklearn` (or other packages). (Note `sklearn` uses BFGS for optimization).\n",
    "Note that the `sklearn` implementation handles floating point underflow/overflow more carefully than we have done, and uses faster numerical optimisation algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(Phi_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_test_pred = clf.predict(Phi_test)\n",
    "accuracy_score(Y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "#### **Exercise 4**\n",
    "Consider how you would extend logistic regression to the multiclass case where $y \\in \\{c_1, \\ldots c_m\\}$. How would you model the conditional probability for class $k$: $p(y=c_k \\vert \\mathbf{x})$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
